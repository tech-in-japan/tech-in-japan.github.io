<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FEDBM7F51Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FEDBM7F51Q');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏠 🤸🏽 🍗 強化ディープラーニングはまだ機能しません 👍🏻 👵🏽 💆</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="著者について 。 Alex Irpanは、GoogleのBrain Roboticsグループの開発者であり、その前は、Berkeley Artificial Intelligence Research（BAIR）研究所で働いていました。 
  
 
  
 過去数年間のバークレー、Google Br...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="sitemap" type="application/xml" href="/sitemap.xml"/>

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

  <script>document.write('<script src="https://pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://tech-in-japan.github.io/index.html"></a>
    <div class="page-header-text">Clever Geek Handbook</div>
  </header>
  <section class="page js-page"><h1>強化ディープラーニングはまだ機能しません</h1><div class="post__text post__text-html js-mediator-article" id="post-content-body" data-io-article-url="https://habr.com/ru/post/349800/">  <a href="https://www.alexirpan.com/about/">著者について</a> 。  Alex Irpanは、GoogleのBrain Roboticsグループの開発者であり、その前は、Berkeley Artificial Intelligence Research（BAIR）研究所で働いていました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>過去数年間のバークレー、Google Brain、DeepMind、およびOpenAIの記事が主に引用されています。これらの記事は私の観点から最も目立つからです。</i>  <i>私はほぼ確実に古い文献や他の組織から何かを見逃したので、私は謝罪します-結局のところ、私はただ一人です。</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee9/625/517/ee96255172175888c7c273e689b71b0b.jpg"></div>
      
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> はじめに </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Facebookで次のことを述べました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote>  <i><font color="gray">誰かが強化学習（RL）で問題を解決できるかどうか尋ねると、すぐには答えられません。</font></i>  <i><font color="gray">これは少なくとも70％のケースで当てはまると思います。</font></i> </blockquote> 強化されたディープラーニングには、大量の誇大宣伝が伴います。 そして、正当な理由があります！ 強化学習（RL）は信じられないほど一般的なパラダイムです。 原則として、信頼性が高く高性能なRLシステムはすべてにおいて完璧でなければなりません。 このパラダイムとディープラーニングの経験的な力の融合は自明です。  Deep RLは最も強力なAIのように見えますが、これは一種の夢であり、数十億ドルの資金調達を促進します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     残念ながら、実際にはこのことはまだ機能しません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     しかし、私は彼女が撃つと信じています。 信じられなかったら、このトピックでは料理をしませんでした。 しかし、先には多くの問題があり、その多くは根本的に複雑です。 訓練されたエージェントの美しいデモは、作成中にこぼれた血、汗、涙をすべて隠します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     数回、最近の結果に人々が誘惑されるのを見ました。 彼らは最初に深いRLを試し、常に困難を過小評価していました。 間違いなく、この「モデルタスク」は見かけほど単純ではありません。 そして、疑いもなく、この分野は彼らが研究に現実的な期待を設定することを学ぶ前に、彼らを数回破りました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     個人的な間違いはありません。 これはシステムの問題です。 ポジティブな結果についてストーリーを描くのは簡単です。 これをネガティブにしてみてください。 問題は、研究者がほとんどの場合正確に否定的な結果を得るということです。 ある意味では、そのような結果は肯定的な結果よりもさらに重要です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     この記事では、ディープRLが機能しない理由を説明します。 私の意見では、それがまだ機能する場合の例と、将来より信頼性の高い作業を達成する方法を示します。 これは、深いRLで作業している人を止めるためではありませんが、誰もが問題を理解していれば進歩しやすいからです。 あなたが本当に問題について話すならば、合意に達するのは簡単です、そして、お互いに別々に同じレーキに何度も何度もつまずかない。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ディープRLのトピックに関する研究をもっと楽しみたいです。 新しい人がここに来るように。 そして、彼らは自分が何に興味を持っているのかを知るように。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     先に進む前に、いくつかの発言をさせてください。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li> いくつかの科学論文がここに引用されています。 私は通常、説得力のあるネガティブな例を挙げ、ポジティブなことについては黙っています。  <b>これは、私が科学研究が好きではないという意味ではありません</b> 。 彼らはすべて良いです-時間があれば読む価値があります。 </li><li> 私の日常業務では、RLは常に深いRLを意味するため、「強化学習」と「深層強化学習」という用語を同義語として使用します。  <b>一般的に強化を伴う学習よりも強化を伴う深層学習の経験的な行動を批判し</b>ます。 引用された記事は通常、ディープニューラルネットワークを持つエージェントの仕事について説明しています。 経験的批判<i>は</i>線形RL（線形RL）または表形式RL（表形式RL）にも当てはまる<i>かもしれ</i>ませんが、この批判をより小さなタスクに拡張できるかどうかはわかりません。 深いRLを取り巻く誇大広告は、RLが、良好な近似関数が必要な大規模で複雑な多次元環境のソリューションとして提示されているという事実によるものです。 特に、この誇大広告でそれを整理する必要があります。 </li><li> この記事は、悲観論から楽観論に移行するように構成されています。 私はそれが少し長いことを知っています、しかし、あなたが答える前にあなたがそれを全部読むのに時間をかけるならば、私は非常に感謝します。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これ以上苦労することなく、ディープRLがクラッシュするいくつかのケースを以下に示します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 強化されたディープラーニングは非常に効果が低い場合があります </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     強化を伴うディープラーニングの最も有名なベンチマークは、Atariゲームです。 よく知られている記事Deep Q-Networks（DQN）に示されているように、Qラーニングを適切なサイズのニューラルネットワークといくつかの最適化のトリックと組み合わせると、いくつかのAtariゲームで人間のパフォーマンスを達成したり、それらを上回ることができます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ゲームAtariゲームは毎秒60フレームで実行されます。 結果を人として表示するために最適なDQNを処理するために必要なフレーム数をすぐに把握できますか？ 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     答えはゲームによって異なるため、最近のDeepmindの記事-Rainbow <a href="https://arxiv.org/abs/1710.02298">DQN（Hessel et al、2017）を</a>ご覧ください。 元のDQNアーキテクチャへの連続した強化のいくつかが結果をどのように改善するかを示し、すべての改善を組み合わせることが最も効果的です。 ニューラルネットワークは、57のアタリゲームのうち40以上で人間の結果を超えています。 結果はこの便利なチャートに示されています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/917/09e/b68/91709eb6826b82f803fdf3c634321592.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     縦軸は、「人間に対して正規化された平均結果の中央値」を示しています。 アタリゲームごとに57個のDQNニューラルネットワークをトレーニングし、人間の結果を100％とするときに各エージェントの結果を正規化し、57ゲームの平均中央値を計算することによって計算されます。  RainbowDQNは、1800 <i>万</i>フレームを処理した後、100％のマイルストーンを超えています。 これは、約83時間のプレイに加えて、どれだけ時間がかかってもトレーニング時間に相当します。 これは、ほとんどの人が数分でつかむ単純なAtariゲームの多くの時間です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     前のレコードは<a href="https://arxiv.org/pdf/1707.06887.pdf">分布DQN</a>システムに属していたため<a href="https://arxiv.org/pdf/1707.06887.pdf">（Bellemare et al、2017）</a> 、100％の結果を達成するには7000万フレーム、つまり約4倍の時間が必要だったため、実際には1800万フレームが非常に良い結果であることに留意してください。  <a href="https://www.nature.com/articles/nature14236">Nature DQN（Mnih et al、2015）</a>に関しては、2億フレーム後でも、100％の中央値に達することはありません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      「計画エラー」の認知バイアスは、タスクの完了に通常予想よりも長い時間がかかることを示しています。 強化学習には独自の計画ミスがあります。通常、トレーニングには思ったよりも多くのサンプルが必要です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     問題はAtariゲームに限定されません。  2番目に人気のあるテストは、MuJoCo物理エンジンのタスクセットであるMuJoCoベンチマークです。 これらのタスクでは、通常、ロボットのシミュレーションにおける各ヒンジの入力と速度が入力で与えられます。 視覚の問題を解決する必要はありませんが、RLシステムは、 <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mn>5</mn></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.379ex" height="2.419ex" viewBox="0 -935.7 1454.9 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700023,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgZw_dUxadwdA3R7tdbSkCSWzve4A#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700023,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgZw_dUxadwdA3R7tdbSkCSWzve4A#MJMAIN-30" x="500" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700023,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgZw_dUxadwdA3R7tdbSkCSWzve4A#MJMAIN-35" x="1415" y="557"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>5</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-1"> 10 ^ 5 </script> 前に <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mn>7</mn></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.379ex" height="2.419ex" viewBox="0 -935.7 1454.9 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700023,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgZw_dUxadwdA3R7tdbSkCSWzve4A#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700023,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgZw_dUxadwdA3R7tdbSkCSWzve4A#MJMAIN-30" x="500" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ja&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700023,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgZw_dUxadwdA3R7tdbSkCSWzve4A#MJMAIN-37" x="1415" y="557"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>10</mn><mn>7</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-2"> 10 ^ 7 </script> タスクに応じた手順。 これは、このような単純な環境での制御には信じられないほどです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     以下に示す<a href="https://arxiv.org/abs/1707.02286">Parkour DeepMindの記事（Heess et al、2017）は</a> 、100人以上の従業員64人を使用してトレーニングされています。 この記事ではワーカーが何であるかを指定していませんが、これは単一のプロセッサーを意味すると想定しています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これは<i>最高の結果</i>です。 彼が最初に出てきたとき、私は深いRLが実行中にそのような歩行を一般的に学ぶことができたことに驚きました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     しかし、プロセッサー時間は6400時間かかり、少し残念です。 期待していた時間が短いわけではありません...単純なスキルでは、深いRLが実際に役立つトレーニングのレベルよりも1桁劣っていることは残念です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     明らかな反論があります。トレーニングの効果を単に無視した場合はどうでしょうか？ エクスペリエンスを簡単に生成できる特定の環境があります。 たとえば、ゲーム。 しかし、これが<i>不可能な</i>環境では、RLは大きな課題に直面します。 残念ながら、ほとんどの環境はこのカテゴリに分類されます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 最終的なパフォーマンスのみに関心がある場合は、他の方法で多くの問題を解決した方がよいでしょう。 </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     問題の解決策を探すとき、通常、さまざまな目標を達成するために妥協点を見つける必要があります。 この特定の問題に対する本当に良い解決策に集中することも、研究全体への最大限の貢献に集中することもできます。 最良の問題は、優れた解決策を得るために研究への貢献が必要な場合です。 しかし実際には、これらの基準を満たす問題を見つけることは困難です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     純粋に最大限の効率を実証するという点で、深いRLは他の方法よりも常に優れているため、それほど印象的な結果を示しません。 インタラクティブなパス最適化によって制御されるMuJoCoロボットのビデオを次に示します。 正しいアクションは、オフライン学習なしで、ほぼリアルタイムでインタラクティブに計算されます。 はい、すべてが2012年の機器で動作します（ <a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Tassa et al、IROS 2012</a> ）。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     この作業は、パルクールに関するDeepMindの記事と比較できると思います。 それらはどう違うのですか？ 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     違いは、ここでは著者が予測モデルで制御を適用し、地球の実際のモデル（物理エンジン）で作業することです。  RLにはそのようなモデルはないため、作業が大幅に複雑になります。 一方、モデルベースの行動計画が結果を大幅に改善するのであれば、なぜRLトレーニングをだましてやる必要があるのでしょうか？ 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     同様に、モンテカルロ（MCTS）ターンキーツリー検索ソリューションを使用すると、AtariのDQNニューラルネットワークを簡単に上回ることができます。 以下は、 <a href="https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning">Guo et al、NIPS 2014</a>の主要な指標です。 著者は、訓練されたDQNの結果をUCTエージェントの結果と比較します（これは最新のMCTSの標準バージョンです）。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/a0f/d12/cd9/a0fd12cd99514b6f090bd71aec4c5e0a.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/b59/52a/d8d/b5952ad8d6d8eca97669f4d6c6c313a1.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     繰り返しますが、これは不公平な比較です。DQNは検索を行わず、MCTSは地球物理学の実際のモデル（Atariエミュレーター）を使用して正確に検索を行うためです。 しかし、状況によっては、気にしない場合があります。これは、正直なまたは不正な比較です。 動作するために必要な場合もあります（完全なUCT評価が必要な場合は、元の科学記事<a href="http://www.marcgbellemare.info/static/publications/bellemare13arcade.pdf">Arcade Learning Environment（Bellemare et al、JAIR 2013</a> ）の付録を参照してください。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     強化学習は、世界の未知のモデルを含む環境を含むすべてに理論的に適しています。 それにもかかわらず、そのような汎用性は高価です。学習に役立つ可能性のある特定の情報を使用することは困難です。 このため、最初からハードコーディングできることを学習するには、多くのサンプルを使用する必要があります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     経験から、まれなケースを除き、特定のタスクに合わせて調整されたアルゴリズムは強化学習よりも高速で優れた動作をすることが示されています。 深在性RLのために深在性RLを開発している場合は問題ではありませんが、個人的には、RL cの有効性を...他の何かと比較するのは気分を害します。  AlphaGoがとても好きだった理由の1つは、それがディープRLにとって明確な勝利だったからです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これらすべての理由から、私のタスクがとてもクールで複雑で面白い理由を人々に説明することはより困難です。なぜなら、彼らはしばしば<i>なぜ</i>難しいの<i>か</i>を評価する<i>ための</i>背景や経験がないからです。 ディープRLの機能について人々が考えることと、実際の機能との間には明確な違いがあります。 今、私はロボット工学の分野で働いています。 ロボット工学について言及する場合、ほとんどの人の頭に浮かぶ会社を考えてみましょう：Boston Dynamics。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     このことは強化トレーニングを使用しません。 ここでRLが使用されていると思った人に何度か会いましたが、いません。 開発者グループから発行された科学論文を探すと、 <a href="https://dspace.mit.edu/openaccess-disseminate/1721.1/110533">線形2次レギュレータ、時変2次計画法ソルバー、凸最適化</a>に関する記事が見つかります。 言い換えれば、彼らは主に古典的なロボット工学の方法を適用します。 これらの古典的な手法は、適切に適用すればうまく機能することがわかりました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 強化トレーニングには通常、報酬機能が必要です </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     強化された学習は、報酬関数の存在を意味します。 通常、最初に存在するか、オフラインモードで手動で構成され、トレーニング中は変更されません。 シミュレーショントレーニングやリバースRL（報酬関数が事後的に復元される場合）などの例外があるため、「通常」と言いますが、ほとんどの場合、RLは報酬としてオラクルとして使用します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      RLが適切に機能するためには、報酬機能が必要なものを<i>正確に</i>カバーする必要があることに注意することが重要です。 そして私は<i>正確</i>に意味し<i>ます</i> 。  RLは煩わしくオーバーフィットしやすいため、予期しない結果につながります。 それが、Atariが非常に優れたベンチマークである理由です。 多くのサンプルを入手するのは簡単なだけでなく、すべてのゲームには明確な目標（ポイント数）があるため、報酬関数を見つけることを心配する必要はありません。 そして、あなたは他の誰もが同じ機能を持っていることを知っています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      MuJoCoタスクの人気は、同じ理由によるものです。 シミュレーションで機能するため、オブジェクトの状態に関する完全な情報が得られるため、報酬関数の作成が大幅に簡素化されます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Reacherタスクでは、中心点に接続された2セグメントのアームを制御します。目標は、アームの端を特定のターゲットに移動することです。 学習の成功例については、以下を参照してください。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/BkhSKqc8vSA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     すべての座標が既知であるため、報酬は手の端からターゲットまでの距離に加えて、移動するための短い時間として定義できます。 原則として、現実の世界では、座標を正確に測定するのに十分なセンサーがあれば、同じ実験を行うことができます。 しかし、システムが何をする必要があるかに応じて、合理的な報酬を決定することは困難です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     報酬関数自体がなければ、大きな問題にはなりません... 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 報酬機能の開発の複雑さ </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     報酬関数を作成することはそれほど難しくありません。 適切な振る舞いを奨励する機能を作成しようとすると困難が発生し、同時にシステムは学習を維持します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      HalfCheetahには、垂直面に囲まれた2本足のロボットがあります。つまり、前方または後方にしか移動できません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <a href=""><img src="https://habrastorage.org/webt/h9/dc/hf/h9dchfesck8yxwcnxibb5n0pdkq.png"></a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     目標はジョギングを学ぶことです。 報酬はHalfCheetahの速度です（ <a href="">ビデオ</a> ）。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これは<i>滑らかな、</i>または形を整えた（形をした）報酬です。つまり、最終目標に近づくにつれて増加します。 目標の最終状態に到達したときにのみ付与される、 <i>まばらな</i>報酬とは対照的に、他の状態では存在しません。 トレーニングが問題の完全な解決策を提供しなかった場合でも、肯定的なフィードバックを提供するため、報酬のスムーズな成長は、多くの場合、習得がはるかに簡単です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     残念ながら、スムーズな成長による報酬には偏りがあります（バイアス）。 すでに述べたように、このため、予期しない望ましくない動作が現れます。 良い例は、 <a href="https://blog.openai.com/faulty-reward-functions/">OpenAIブログ記事の</a>ボートレースです。 目標はゴールに到達することです。 与えられた時間にレースが終了すると+1の報酬を、それ以外の場合は0の報酬を想像できます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     報酬機能は、チェックポイントを通過するためのポイントと、フィニッシュラインにすばやく到達できるボーナスを収集するためのポイントを提供します。 判明したように、ボーナスを集めることはレースの完了よりも多くのポイントを与えます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     正直なところ、この出版物は最初は少し面倒でした。 彼女が間違っているからではありません！ しかし、彼女は明らかなことを実証しているように思えたからです。 もちろん、報酬が誤って定義されている場合、強化学習は奇妙な結果をもたらします！ この出版物はこの特定のケースを不当に重要視しているように思えました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     しかし、その後、私はこの記事の執筆を開始し、誤って定義された報酬の最も説得力のある例が、 <i>まさに</i>このボートレースのビデオであることに気付きました。 それ以来、このトピックに関するいくつかのプレゼンテーションで使用され、問題に注目を集めました。 それでいいのですが、私はしぶしぶ良いブログ投稿だったことを認めます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      RLアルゴリズムは、周囲の世界について多少なりとも推測する必要がある場合、ブラックホールに陥ります。 モデルレスRLの最も汎用性の高いカテゴリは、ブラックボックス最適化のようなものです。 そのようなシステムは、それらがMDP（Markov意思決定プロセス）にあると仮定することだけが許可されます-それ以上はありません。 エージェントは、これはあなたが+1を得るものであると単純に言われますが、あなたはこのためにそれを得るのではなく、あなた自身で他のすべてを見つけるべきです。 ブラックボックスの最適化と同様に、問題は、報酬が間違った方法で受け取られた場合でも、+ 1を与える動作は良いと見なされることです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     古典的な例はRL分野のものではありません-誰かが超小型回路の設計に遺伝的アルゴリズムを適用<a href="https://en.wikipedia.org/wiki/Evolvable_hardware">し、最終設計に1つの接続さ</a>れてい<a href="https://en.wikipedia.org/wiki/Evolvable_hardware">ない論理ゲートが必要な</a>回路を受け取ったときです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/cb1/492/23a/cb149223a5dec3a7d3c9051d63a0334e.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><font color="gray">灰色の要素は、左上隅の要素を含む回路の正しい動作に必要ですが、何にも接続されていません。</font></i>  <i><font color="gray">記事<a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.50.9691%26rep%3Drep1%26type%3Dpdf">「物理学に絡み合ったシリコンに固有の進化した回路」から</a></font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     または、より最近の例として、 <a href="https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/">2017 Salesforceブログ投稿があり</a>ます。 彼らの目標は、テキストの要約を作成することでした。 基本モデルは教師に教えられた後、ROUGEと呼ばれる自動化されたメトリックによって評価されました。  ROUGEは差別化されていない報酬ですが、RLはそのようなものに対応できます。 そこで、彼らはRLを適用してROUGEを直接最適化しようとしました。 これで高いROUGE（歓声！）が得られますが、歌詞はあまり良くありません。 以下に例を示します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote>  <i>バトンは、ERSが彼をスタートさせなかった後、マクラーレンの100回目のレースを彼から奪いました。</i>  <i>英国人にとっては悪い週末を終えた。</i>  <i>資格に先んじるボタン。</i>  <i>バーレーンのニコ・ロズベルグに先行してフィニッシュ。</i>  <i>ルイス・ハミルトン。</i>  <i>11レースで..レース。</i>  <i>2,000ラップをリードするために...で... I.- <a href="https://arxiv.org/abs/1705.04304">Paulus et al、2017</a></i> </blockquote>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     そして、RLモデルは最大のROUGE結果を示しましたが... 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/d78/d17/d85/d78d17d851b470c97d9a37e98b12accd.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      ...彼らは最終的に、履歴書を書くために別のモデルを使用することにしました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     別の楽しい例。 これは、「折り畳み式レゴコンストラクターに関する記事」としても知られている<a href="https://arxiv.org/abs/1704.03073">Popov et al、2017の</a>記事によるものです。 著者は、DDPGの分散バージョンを使用してキャプチャルールを教えています。 目標は、赤い立方体をつかみ、青の上に置くことです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     彼らは彼女の作品を作ったが、失敗の興味深いケースに直面した。 最初のリフティング動作は、赤いブロックのリフティング高さに基づいて報われます。 これは、立方体の底面のZ座標によって決まります。 失敗オプションの1つでは、モデルは底面を上にして赤い立方体をオンにし、上げないようにすることを学びました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/8QnD8ZM0YCo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     明らかに、この動作は意図されたものではありません。 しかし、RLは気にしません。 強化訓練の観点から、彼女は立方体を回すことに対する報酬を受け取ったので、彼女は立方体を回し続けます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     この問題を解決する1つの方法は、キューブを接続した後にのみ報酬を与えることで、報酬をまばらにすることです。 まれな報酬が学習に役立つため、これが機能する場合があります。 しかし、多くの場合、これはそうではありません。積極的な強化が不足しているため、事態は複雑になりすぎています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     問題の別の解決策は、報酬の慎重な形成、新しい報酬条件の追加、およびトレーニング中にRLアルゴリズムが望ましい動作を示すまで既存の条件の係数の調整です。 はい、この面でRLを克服する<i>ことは可能ですが</i> 、そのような闘争は満足をもたらしません。 時にはそれが必要ですが、その過程で何かを学んだとは感じませんでした。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     参考までに、レゴコンストラクターの折りたたみに関する記事の報酬関数の1つを次に示します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/cea/4f1/c40/cea4f1c405600c81096b8c953d37de50.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     この関数の開発にどれだけの時間を費やしたかはわかりませんが、メンバーの数と異なる係数で「たくさん」と言います。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     他のRL研究者との会話の中で、誤って設定された報酬を持つモデルの元の行動に関するいくつかの話を聞きました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li> 同僚がエージェントに部屋を移動するように教えます。 エージェントが国境を越えるとエピソードは終了しますが、この場合罰金は科されません。 トレーニングの終わりに、エージェントは自殺行動を採用しました。これは、負の報酬を得るのが非常に簡単で、正の報酬が難しすぎるため、結果が0の迅速な死亡が、負の結果のリスクが高い長寿命よりも望ましいからです。 </li><li> 友人がロボットアームシミュレーターを訓練して、テーブルの上の特定のポイントに向かって移動しました。 ポイントは<i>テーブルに対して相対的に</i>定義され、テーブルは何にも接続されていなかったことがわかります。 モデルはテーブルを非常に激しくノックすることを学び、テーブルを倒して目標点を動かしました-そしてそれは手の隣にありました。 </li><li> 研究者は、ハンマーを使って釘を打つロボットアームのシミュレーターを訓練するためのRLの使用について話しました。 報酬はもともと、釘が穴にどれだけ入ったかによって決まりました。 ロボットはハンマーを拾う代わりに、手足で釘を打ちました。 それから彼らはロボットにハンマーを拾うように奨励する報酬を追加しました。 その結果、ロボットの学習戦略はハンマーを取り、ツールを釘に投げ込み、通常の方法では使用しないことでした。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     確かに、これはすべて間違った唇からの物語です。個人的に、私はこの行動をしたビデオを見たことはありません。 しかし、これらの物語はどれも私には不可能ではないようです。 私はRLで何度も火傷を負い、信じられませんでした。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">ペーパーオプティマイザー</a>に関する話をしたい人を知っています。 さて、私は正直に理解しています。 しかし、実際には、私はこれらの話を聞くのにうんざりしています。なぜなら、彼らは常に本当の話としての超人的な混乱した強いAIについて話しているからです。 周りに毎日たくさんの実際の物語があるのに、なぜそれを発明するのか。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 良い報酬を与えられたとしても、局所的な最適を避けることは困難です。 </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     以前のRLの例は、多くの場合「報酬ハック」と呼ばれます。<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">私にとっては、これはスマートで非標準のソリューションであり、タスクデザイナーから期待されるソリューションよりも多くの報酬をもたらします。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ハッキング報酬は例外です。</font><font style="vertical-align: inherit;">より一般的なのは、探査と開発の間の誤った妥協から生じる誤った局所的最適の場合です。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは私のお気に入りのビデオの1つです</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">HalfCheetahで学習</font><font style="vertical-align: inherit;">する</font></font><a href="https://arxiv.org/abs/1603.00748"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">正規化</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">された</font><a href="https://arxiv.org/abs/1603.00748"><font style="vertical-align: inherit;">特典機能</font></a><font style="vertical-align: inherit;">を</font><font style="vertical-align: inherit;">実装</font><font style="vertical-align: inherit;">します。</font><font style="vertical-align: inherit;">部外者の観点から、これは非常に、</font><i><font style="vertical-align: inherit;">非常に</font></i></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <a href=""><img src="https://habrastorage.org/webt/h9/dc/hf/h9dchfesck8yxwcnxibb5n0pdkq.png"></a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バカ。</font><font style="vertical-align: inherit;">しかし、私たちは、私たちが横から見て、あなたの足で動くことはあなたの背中に横たわるよりも優れているという多くの知識を持っているという理由だけで愚かだと言います。</font><font style="vertical-align: inherit;">RLはこれを知りません！</font><font style="vertical-align: inherit;">彼は状態ベクトルを見て、アクションベクトルを送信し、肯定的な報酬を受け取っていることを確認します。</font></font>以上です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> トレーニング中に何が起こったかについて私が思いつくことができる最も妥当な説明を以下に示します。 </font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ランダムな研究で、このモデルは、動きを止めないよりも前に倒す方が収益性が高いことを発見しました。 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> モデルは、このような動作を「フラッシュ」して継続的に低下し始めるのに十分な頻度でこれを実行しました。 </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 前方に倒れた後、モデルは、十分な力を加えると、バックフリップを行うことができ、それがもう少しの報酬を与えることを学びました。 </font></font></li><li>      —  ,    ,      «»   . </li><li>     ,     —       « »    ,   ?      . </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは非常におもしろいですが、明らかにロボットに望むものではありません。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">失敗した別の例を次に示します。今回はReacher（</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">video</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）に</font><font style="vertical-align: inherit;">囲まれ</font><font style="vertical-align: inherit;">ています。今回の実行では、</font><font style="vertical-align: inherit;">通常</font><font style="vertical-align: inherit;">、ランダムな初期重みがアクションに対して強い正または非常に負の値を与えました。このため、ほとんどのアクションは可能な限り最大または最小の加速で実行されました。実際、モデルを非常に簡単にスピンアップできます。各ヒンジに大きな力を加えるだけです。ロボットが回転するとき、この状態から何らかの理解可能な方法で抜け出すことはすでに困難です：横行回転を停止するために、いくつかの偵察手順が取られるべきです。もちろん、これは可能です。しかし、これは今回の実行では発生しませんでした。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <a href=""><img src="https://habrastorage.org/webt/fd/db/mx/fddbmxo99tsxafisrm_sugkemzs.png"></a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">どちらの場合も、古典的な偵察/搾取の問題があります。これは、太古から、強化された学習を追求してきました。データは現在のルールから派生しています。現在のルールが広範なインテリジェンスを提供している場合、不要なデータを受け取り、何も学習しません。エクスプロイトが多すぎる-最適でない動作を「縫う」。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">このテーマには直感的に楽しいアイデアがいくつかあります。内部の動機と好奇心、カウントに基づく知性などです。これらのアプローチの多くは、80年代以前に最初に提案され、一部はディープラーニングモデル用に改訂されました。しかし、私の知る限り、すべての環境で安定して機能するアプローチはありません。時には役立つこともあれば、役に立たないこともあります。ある種のインテリジェンストリックがどこでも機能するのは良いことですが、近い将来、彼らがこの口径の特効薬を見つけるとは思いません。誰も試みていないからではなく、探査-開発が非常に、非常に、非常に、非常に複雑な問題だからです。多腕</font></font><a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">バンディットに関するウィキペディアの記事からの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">引用</font><font style="vertical-align: inherit;">：</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">歴史上初めて、この問題は第二次世界大戦の連合国の科学者によって研究されました。</font><font style="vertical-align: inherit;">ピーター・ホイットルによると、ドイツの科学者もそれに時間を費やすために、ドイツ人に投げつけられるべきであることが示唆されたほど、それは非常に扱いにくいことが判明しました。</font></font></i> </blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（情報源：</font></font><a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.57.1916%26rep%3Drep1%26type%3Dpdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Q-Learning for Bandit</font></font></a><font style="vertical-align: inherit;"></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Problem </font><a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.57.1916%26rep%3Drep1%26type%3Dpdf"><font style="vertical-align: inherit;">、Duff 1995</font></a><font style="vertical-align: inherit;">）</font><font style="vertical-align: inherit;">私はあなたの報酬を故意に誤解し、ローカル最適を達成するための最も怠ziな方法を積極的に探している悪魔として深いRLを提示します。</font><font style="vertical-align: inherit;">少しばかげているが、それは本当に生産的な思考であることが判明した。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ディープRLが機能する場合でも、奇妙な動作に再トレーニングできます。 </font></font></h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ディープラーニングは、テストスイートで学習することが社会的に受け入れられる唯一の機械学習領域であるため、人気があります。</font></font></i> </blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（</font></font><a href="https://twitter.com/jacobandreas/status/924356906344267776"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">出典</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">強化トレーニング</font><font style="vertical-align: inherit;">の</font><font style="vertical-align: inherit;">プラス面は、特定の環境で良い結果を達成したい場合、狂人として再トレーニングできることです。欠点は、モデルを他の環境に拡張する必要がある場合、恐ろしい再訓練のためにおそらくうまく機能しないことです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DQNネットワークは、多くのAtariゲームに対応しています。これは、各モデルのすべてのトレーニングが、1つのゲームで最大の結果を達成するという単一の目標に焦点を合わせているためです。最終モデルは他のゲームに拡張することはできません。なぜなら、それはそのように教えられていなかったからです。新しいAtariゲームのトレーニング済みDQNを構成できます（</font></font><a href="https://arxiv.org/abs/1606.04671"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">プログレッシブニューラルネットワーク（Rusu et al、2016）を参照）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）、しかし、そのような転送が行われるという保証はなく、通常誰もこれを期待していません。これは、ImageNetの事前にトレーニングされた機能で人々が目にする大成功ではありません。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">いくつかの明白なコメントを防ぐために：はい、原則として、幅広い環境でのトレーニングはいくつかの問題を解決できます。場合によっては、モデルのアクションのこのような拡張は単独で発生します。例はナビゲーションです。そこでは、ターゲットのランダムな位置を試して、汎用関数を使用して一般化できます。 （</font></font><a href="http://proceedings.mlr.press/v37/schaul15.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Universal Value Function Approximators、Schaul et al、ICML 2015を参照</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）この作業は非常に有望であると思いますが、後でこの作業からさらに例を示します。しかし、ディープRLを一般化する可能性は、さまざまなタスクセットに対処するほど大きくないと思います。認識はずっと良くなりましたが、「管理のためのImageNet」が登場する前に、深いRLはまだ先を行っています。 OpenAIユニバースはこのき火に火をつけようとしましたが、聞いたところによると、このタスクは難しすぎてほとんど何もしませんでした。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">モデルを一般化するそのような瞬間はありませんが、私たちは驚くほど狭いモデルの範囲に留まっています。例として（そして自分の仕事を笑う言い訳として）、</font><a href="https://arxiv.org/abs/1711.02301"><font style="vertical-align: inherit;">Can Deep RL Solve Erdos-Selfridge-Spencer Gamesの</font></a><font style="vertical-align: inherit;">記事を見てください</font></font><a href="https://arxiv.org/abs/1711.02301"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。 （Raghu et al、2017）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。最適なゲームのための分析形式での解決策がある2人のプレーヤーのための組み合わせゲームを研究しました。最初の実験の1つでは、プレーヤー1の動作を記録し、RLを使用してプレーヤー2を訓練しました。この場合、プレーヤー1のアクションを環境の一部と見なすことができます。最適なプレーヤー1に対してプレーヤー2を教えると、RLが高い結果を示すことができることが示されました。しかし、最適でないプレーヤー1に同じルールを適用すると、最適でないプレーヤーには適用されなかったため、プレーヤー2の有効性が低下しました。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">記事の著者</font></font><a href="https://arxiv.org/abs/1711.00832"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lanctot et al、NIPS 2017</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">同様の結果が得られました。</font><font style="vertical-align: inherit;">ここでは、2人のエージェントがレーザータグを再生します。</font><font style="vertical-align: inherit;">エージェントは、マルチエージェント強化トレーニングを使用してトレーニングされます。</font><font style="vertical-align: inherit;">一般化をテストするために、5つのランダムな開始点（sid）からトレーニングが開始されました。</font><font style="vertical-align: inherit;">これは、互いに対戦するように訓練されたエージェントのビデオです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/8vXpdHuoQH8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">あなたが見ることができるように、彼らは接近してお互いに撃つことを学びました。</font><font style="vertical-align: inherit;">その後、著者はある実験からプレーヤー1を取り出し、別の実験からプレーヤー2と一緒に連れてきました。</font><font style="vertical-align: inherit;">学習したルールを一般化すると、同様の動作が見られるはずです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ネタバレ：彼には会わない。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/jOjwOkCM_i8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは一般的なマルチエージェントRL問題のようです。エージェントが互いに訓練されると、一種の共同進化が起こります。エージェントは本当にお互いに本当によく戦うように訓練されていますが、彼らが以前に会ったことのないプレーヤーに対して彼らが送られるとき、彼らの有効性は減少します。これらのビデオの唯一の違いはランダムシードであることに注意してください。同じ学習アルゴリズム、同じハイパーパラメーター。動作の違いは、純粋に初期条件のランダムな性質によるものです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">それにもかかわらず、互いに独立した遊びがある環境で得られたいくつかの印象的な結果があります-それらは一般的な論文と矛盾するようです。 OpenAIブログには、</font><a href="https://blog.openai.com/competitive-self-play/"><font style="vertical-align: inherit;">この分野での彼らの仕事についての良い投稿</font></a><font style="vertical-align: inherit;">があり</font></font><a href="https://blog.openai.com/competitive-self-play/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">DIYプレイもAlphaGoとAlphaZeroの重要な部分です。</font><font style="vertical-align: inherit;">私の直感的な考えは、エージェントが同じペースで学習すれば、常に競い合って互いの学習をスピードアップできますが、一方が他方よりもはるかに速く学習した場合、彼も弱者の脆弱性を悪用して再訓練します。</font><font style="vertical-align: inherit;">対称的なスタンドアロンゲームから一般的なマルチエージェント設定に移行すると、トレーニングが同じ速度であることを確認するのが非常に難しくなります。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 一般化を考慮しなくても、最終結果が不安定で再現が難しいことが判明する場合があります。 </font></font></h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ほとんどすべての機械学習アルゴリズムには、学習システムの動作に影響するハイパーパラメーターがあります。多くの場合、手動またはランダム検索によって選択されます。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">教師のトレーニングは安定しています。データセットを修正し、真のデータでチェックします。ハイパーパラメーターをわずかに変更しても、機能はあまり変化しません。すべてのハイパーパラメーターが適切に機能するわけではありませんが、長年にわたって多くの経験的なトリックが見つかっているため、多くのハイパーパラメーターはトレーニング中に生命の兆候を示します。人生のこれらの兆候は非常に重要です。彼らはあなたが正しい道を進んでおり、合理的なことをしていると言います-そしてあなたはより多くの時間を費やす必要があります。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">現在、ディープRLはまったく安定しておらず、研究プロセスで非常に迷惑です。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Google Brainで働き始めたとき、私はすぐに上記の記事</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Normalized Advantage Function</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">（NAF）</font><font style="vertical-align: inherit;">からアルゴリズムを実装し始めました</font><font style="vertical-align: inherit;">。 2〜3週間で済むと思いました。私はいくつかの切り札がありました：Teano（TensorFlowによく移植されています）に精通している人、深いRLでの経験、NAF記事の筆頭著者がBrainにインターンしているので、質問で彼を悩ませることができました。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最終的に、ソフトウェアのいくつかのバグのために、結果を再現するのに6週間かかりました。問題は、なぜこれらのバグが長い間隠れていたのかということです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この質問に答えるために、OpenAIジムで最も単純な連続管理タスクである振り子タスクを検討してください。この問題では、振り子が特定のポイントに固定され、重力がそれに作用します。入力は3次元の状態です。作用空間は一次元です。それは振り子に加えられる力の瞬間です。目標は、正確に垂直位置で振り子のバランスをとることです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは小さな問題であり、明確に定義された報酬のおかげでさらに簡単になります。報酬は振り子の角度に依存します。振り子を垂直位置に戻すアクションは、報酬を与えるだけでなく、それを</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">増やし</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ます。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ここにある</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ビデオモデル</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">で、</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ほとんどは、</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">動作します。振り子を正確な垂直位置に移動させるわけではありませんが、重力を補正するための正確な力のモーメントを提供します。</font><font style="vertical-align: inherit;">そして、すべてのエラーを修正した後のパフォーマンスグラフです。各行は、10回の独立した実行の1つからの報酬曲線です。同じハイパーパラメーターで、違いはランダムな開始点のみです。</font><font style="vertical-align: inherit;">10回の実行のうち7回がうまくいきました。 3つは通過しませんでした。</font><i><font style="vertical-align: inherit;">30％の故障率は運用可能と見なされます</font></i><font style="vertical-align: inherit;">。これは、</font><a href="https://arxiv.org/abs/1605.09674"><font style="vertical-align: inherit;">変分情報の最大化調査（Houthooft et al、NIPS 2016）の</font></a><font style="vertical-align: inherit;">別のグラフ</font><font style="vertical-align: inherit;">です。水曜日-ハーフチーター。詳細はそれほど重要ではありませんが、賞はまばらにされました。 y軸は一時的な報酬、x軸はタイムスライスの数、使用されるアルゴリズムはTRPOです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <a href=""><img src="https://habrastorage.org/webt/wf/sn/yu/wfsnyuqnmfbdgufwddmrb72frc0.png"></a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/9c8/4f6/245/9c84f6245bb0ebb035aa881066f511b0.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><a href="https://arxiv.org/abs/1605.09674"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/e9d/581/277/e9d5812774c2d76e35b741a7263b1eb6.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">暗い線は10個のランダムsidのパフォーマンスの中央値であり、影付きの領域は25パーセンタイルから75パーセンタイルまでのカバレッジです。誤解しないでください、このグラフはVIMEの良い議論のようです。しかし、一方で、25パーセンタイルの線は本当にゼロに近いです。これは、開始点がランダムであるために約25％が機能しないことを意味します。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">先生と一緒に教えることにも違いがありますが、それほど悪くはありません。ランダムsidを使用した実行の30％でトレーニングコードが教師に対応していなかった場合、データのロード中またはトレーニング中に何らかのエラーが発生したことは間違いありません。補強付きのトレーニングコードがランダム性よりもうまく対処できない場合、それがバグなのか、それとも悪いハイパーパラメーターなのか、私は運が悪いのかわかりません。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/bee/552/42f/bee55242f3636086cbdb999e784cc82f.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">これは、</font></font><a href="http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「なぜ機械学習が「難しい」のか」</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">という記事の説明</font><a href="http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html"><font style="vertical-align: inherit;">です</font></a><font style="vertical-align: inherit;">。主要な点は、機械学習がクラッシュスペースに追加の次元を追加し、クラッシュオプションの数を指数関数的に増やすことです。ディープRLは、ランダム性という別の次元を追加します。そして、ランダム性の問題を解決する唯一の方法は、ノイズを除去することです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学習アルゴリズムのサンプリングが非効率的で、同時に安定していない場合、研究の生産性が大幅に低下します。</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">たぶん彼は100万歩しか必要ないでしょう。しかし、これに5つのランダムな入力値を掛けてから、ハイパーパラメーターの広がりを掛けると、仮説を効果的にテストするために必要な計算が指数関数的に増加します。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote> <i>それが簡単になったら、私はこれをしばらくやっています-そして、先週約6を費やして、モデルグラデーションをゼロから取得しました。これは、RLタスクの束でケースの50％で動作します。</i>  <i>そして、私はGPUクラスターと、毎日昼食をとる数人の友人がいます。そして、彼らはこの数年間、この分野で働いています。</i> <i>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </i>  <i>さらに、畳み込みニューラルネットワークの正しい設計に関する教師とのトレーニング分野から得られた情報は、主な制限がクレジットの分配/ビットレート制御であり、効果的なプレゼンテーションの欠如ではないため、強化されたトレーニングの分野に拡張されないようです。</i>  <i>ResNet、batchnorm、および非常に深いネットワークはここでは機能しません。</i> <i>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </i>  <i>[教員養成]働きたい。</i>  <i>何かを台無しにしたとしても、通常は何らかのランダムでない結果が得られます。</i>  <i>RLを機能させる必要があります。</i>  <i>何かを台無しにしたり、十分な設定を行わなかったりすると、ほぼ確実にランダムなルールよりも悪いルールを取得することになります。</i>  <i>そして、すべてが完全に調整されていても、悪い結果はケースの30％にあります。</i>  <i>なんで？</i>  <i>はい、そのように。</i> <i>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </i>  <i>要するに、あなたの問題は、「ニューラルネットワークの設計」の複雑さよりも、深いRLの複雑さによる可能性が高いです。</i>  <a href="https://news.ycombinator.com/item%3Fid%3D13519044">-OpenAIで働いていたAndrej KarpathyによるHacker Newsに対するコメント</a> </blockquote>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ランダムシードの不安定性は、炭鉱のカナリアのようなものです。 単純な偶然の一致が実行間のこのような強い違いにつながる場合は、実際のコードの変更による違いを想像してください。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     幸いなことに、この思考実験はすでに実施されており、記事<a href="https://arxiv.org/abs/1709.06560">「Deep Reinforcement Learning That Matters」（Henderson et al、AAAI 2018）で</a>説明されているため、この思考実験を行う必要はありません。 結論は次のとおりです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li> 定数に報酬を乗算すると、パフォーマンスに大きな違いが生じる可能性があります。 </li><li>  5つのランダムシード（レポートの一般的なメトリック）は、重要な結果を示すのに十分ではない場合があります。慎重に選択すると、信頼区間が重複しないためです。 </li><li> 同じアルゴリズムの異なる実装は、同じハイパーパラメーターであっても、同じタスクのパフォーマンスが異なります。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     私の理論では、データは常にインターネットで収集され、制御される唯一のパラメーターは報酬のサイズであるため、RLは初期化と教育プロセスのダイナミクスの両方に非常に敏感です。 良い学習例にランダムに出会う​​モデルは、はるかにうまく機能します。 モデルに良い例が見当たらない場合、何も学習していない可能性があります。これは、偏差が決定的でないとますます確信しているためです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> しかし、ディープRLのすべてのすばらしい成果についてはどうでしょうか。 </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     もちろん、徹底的な強化学習はいくつかの優れた結果を達成しています。  DQNはもはや目新しいものではありませんが、かつてはまったく<i>おかしな</i>発見でした。 同じモデルは、各ゲームを個別に調整することなく、ピクセルによって直接研究されます。  AlphaGoとAlphaZeroも非常に印象的な成果を残しています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     しかし、これらの成功に加えて、深層RLが現実の世界にとって実用的な価値がある場合を見つけることは困難です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     現実の世界でディープRLを実際のタスクに使用する方法を考えてみました-これは驚くほど難しいことです。 私は推奨システムでいくらかの用途を見つけると思ったが、私の意見では、 <a href="https://en.wikipedia.org/wiki/Collaborative_filtering">協調フィルタリング</a>と<a href="https://research.yahoo.com/publications/5863/contextual-bandit-approach-personalized-news-article-recommendation">コンテキストバンディットが</a>依然としてそこを支配している。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     私が最終的に見つけた最高のものは、2つのGoogleプロジェクトでし<a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">た。データセンターのエネルギー消費を削減すること</a>と、最近発表された<a href="https://cloud.google.com/automl/">AutoML Vision</a>プロジェクトです。  OpenAIのジャック・クラークは<a href="https://twitter.com/jackclarkSF/status/919584404472602624">読者に同様の質問をツイートし、同じ結論に達しました</a> 。  （昨年、AutoMLの発表前にツイート）。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      NIPSで無人のレーシングカーの小さなモデルを見せ、彼のために深いRLシステムを開発したと言ったので、アウディは深いRLで何か面白いことをしていることを知っています。  <a href="https://arxiv.org/abs/1706.04972">大きなテンソルグラフのデバイスの配置</a>を<a href="https://arxiv.org/abs/1706.04972">最適化する</a>ための巧みな作業が行われていることを知ってい<a href="https://arxiv.org/abs/1706.04972">ます（Mirhoseini et al、ICML 2017）</a> 。  Salesforceには、RLをかなり慎重に使用する場合に機能するテキスト要約モデルがあります。 この記事を読んでいる間、金融会社はおそらくRLを実験していますが、これについてはまだ証拠がありません。  （もちろん、金融会社には市場で遊ぶ方法を隠す理由があるため、確固たる証拠を得ることができません）。  Facebookは、チャットボットとスピーチのディープRLに最適です。 歴史上のすべてのインターネット企業は、おそらくRLを広告モデルに導入することを考えたことがありますが、誰かが実際にRLを実装した場合、それについては沈黙しています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     だから、私の意見では、徹底的なRLはまだ学術研究のトピックであり、広範囲に使用するための信頼できる技術ではなく、実際に効果的に機能させることができます-そして成功した人は情報を開示しません。 最初の選択肢の方が可能性が高いと思います。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     画像の分類の問題で私に来た場合、事前に訓練されたImageNetモデルをお勧めします-彼らはおそらく完璧に仕事をするでしょう。 私<a href="https://habrahabr.ru/post/331740/">たちは、シリコンバレーシリーズの映画製作者が冗談めかしてホットドッグを認識するための本当のAIアプリケーションを作って</a>いる世界に住んでいます。 ディープRLの同じ成功については言えません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> これらの制限がある場合、ディープRLを使用するタイミングは？ </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これは先験的に難しい質問です。 問題は、同じRLアプローチを異なる環境に適用しようとしていることです。 常に機能するとは限らないのは当然です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     上記に基づいて、強化学習の既存の成果からいくつかの結論を引き出すことができます。 これらは、ディープRLが何らかの質的に印象的な動作を学習するか、この分野の以前のシステムよりもよく学習するプロジェクトです（ただし、これらは非常に主観的な基準です）。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     現時点での私のリストです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li> 前のセクションで言及したプロジェクト：DQN、AlphaGo、AlphaZero、パルクールボット、データセンターのエネルギー消費量の削減、およびNeural Architecture SearchによるAutoML。 </li><li>  <a href="https://blog.openai.com/dota-2/">OpenAI Dota 2 1v1 Shadow Fiendボット</a> 。 <a href="https://blog.openai.com/dota-2/">簡素化された戦闘設定で最高のプロを倒します</a> 。 </li><li>  <a href="https://arxiv.org/abs/1702.06230">スーパースマッシュブラザーズメレーボット</a> 、 <a href="https://www.youtube.com/watch%3Fv%3DdXJUlqBsZtE">1v1ファルコンの</a>プロプレイヤーを<a href="https://www.youtube.com/watch%3Fv%3DdXJUlqBsZtE">同じよう</a>に倒すことができます。  （Firoiu et al、2017）。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      （最近の余談：機械学習は最近、プロの無制限のテキサスホールデムプレーヤーを打ち負かしました。このプログラムは、 <a href="https://www.ijcai.org/proceedings/2017/0772.pdf">Libratus（Brown et al、IJCAI 2017）</a>と<a href="https://arxiv.org/abs/1701.01724">DeepStack（Moravčíket al、2017）の</a>両方を使用します。ディープRL。両方のシステムは非常に優れていますが、強化されたディープラーニングを使用せず、反省の反事実的最小化のアルゴリズムとサブゲームの有能な反復ソリューションを使用します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     このリストから、学習を促進する共通のプロパティを分離できます。 以下にリストされているプロパティはいずれもトレーニングに<i>必要</i>では<i>あり</i>ませんが、存在するほど、結果は確実に良くなります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b>ほぼ無制限のエクスペリエンスの簡単な生成</b> 。 ここで利点は明らかです。 データが多いほど、トレーニングが容易になります。 これは、アタリ、ゴー、チェス、将gi、およびシミュレートされたパルクールボット環境に適用されます。 これはおそらく、データセンターの電源プロジェクトにも当てはまります。これ<a href="https://googleblog.blogspot.com/2014/05/better-data-centers-through-machine.html">は、以前の研究（Gao、2014）</a>で、ニューラルネットワークがエネルギー効率を高精度で予測できることが示されたためです。 このようなシミュレーションモデルを使用して、RLシステムをトレーニングします。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     おそらくこの原則はDota 2とSSBMでの動作に適用されますが、ゲームの最大速度とプロセッサの数に依存します。 </li><li>  <b>タスクはより単純な形式に簡素化され</b>ます。 ディープRLで最もよくある間違いの1つは、野心的な計画と夢です。 強化学習は何でも可能です！ しかし、これは一度にすべてを引き受ける必要があるという意味ではありません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      OpenAI Dota 2ボットは、ゲームの開始時にのみ動作し、Shadow Fiend対Shadow Fiend、1x1のみ、特定の設定、固定位置および建物タイプで動作し、おそらく<a href="https://developer.valvesoftware.com/wiki/Dota_2_Workshop_Tools/Scripting/API">Dota 2 API</a>を使用してグラフィック処理タスクを解決しません。  SSBMボットは超人的なパフォーマンスを発揮しますが、1x1ゲームでのみ、キャプテンファルコンとのみ、無限の試合時間を持つBattlefieldでのみです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これはボットのbot笑ではありません。 実際、単純な問題が解決されているかどうかさえわからないのに、なぜ複雑な問題を解決するのですか？ この分野での一般的なアプローチは、最初にコンセプトの最小限の証明を得て、後でそれを一般化することです。  OpenAIはDota 2の作業を拡大します。作業はSSBMボット<a href="https://github.com/vladfi1/phillip">を他のキャラクター</a>に拡大する作業を続けています。 </li><li>  <b>独立したゲームで学ぶ方法があり</b>ます。 これが、AlphaGo、AlphaZero、Dota 2 Shadow Fiend、SSBM Falconボットの仕組みです。 独立したゲームとは競争ゲームを意味しますが、両方のプレイヤーは1人のエージェントで制御できます。 どうやら、この設定は最も安定した結果をもたらします。 </li><li>  <b>学習に対する適切な報酬を決定する明確な方法があり</b>ます。  2人用のゲームでは、これは勝つと+1、負けると-1です。  <a href="https://openreview.net/forum%3Fid%3Dr1Ue8Hcxg">Zophらによる元のNeural Architecture Searchの記事、ICLR 2017</a>では、訓練されたモデルの検証は正確でした。 スムーズな報酬を求めるたびに、間違った目標に合わせてモデルを最適化する最適でないポリシーを学習する機会を導入します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     適切な報酬を得る方法について詳しく知りたい場合は、「 <a href="https://en.wikipedia.org/wiki/Scoring_rule">正しいスコアリングルール</a> 」というフレーズを検索してみてください。  <a href="https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/">このTerrence Taoのブログ投稿</a>は、アクセシブルな例を提供しています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     学習に関しては、何が効果的で何が効果的でないかを試してみる以外にアドバイスはありません。 </li><li>  <b>継続的な報酬が決定される場合、少なくともそれは豊かでなければなりません</b> 。  Dota 2では、最新のヒット（プレイヤーによって殺されたモンスターごと）とヘルス（正確な攻撃またはスキルの適用後にトリガーされる）に対して報酬を与えることができます。 これらの信号は迅速かつ頻繁に到着します。  SSBMボットは、攻撃が成功するたびに信号で、行われ、受けたダメージに対して報酬を受け取ることができます。 アクションと結果の間の遅延が短いほど、フィードバックループのクローズが速くなり、強化システムが最大の報酬への道を見つけるのが容易になります。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 例：ニューラルアーキテクチャ検索 </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     いくつかの原則を組み合わせて、Neural Architecture Searchの成功を分析できます。  <a href="https://arxiv.org/abs/1611.01578">ICLR 2017の</a>元の<a href="https://arxiv.org/abs/1611.01578">バージョンに</a>よると、12,800のサンプルの後、ディープRLはその種の最高のニューラルネットワークアーキテクチャを設計できます。 確かに、各例では、ニューラルネットワークを収束するようにトレーニングする必要がありましたが、サンプルの数では依然として非常に効果的です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     上記のように、報酬は検証の正確さです。 これは非常に豊富な報酬信号です。ニューラルネットワークの構造の変更によって精度が70％から71％に向上した場合でも、RLはその恩恵を受けます。 さらに、ディープラーニングのハイパーパラメーターが線形独立に近いという証拠があります。  （これは、 <a href="https://arxiv.org/abs/1706.00764">ハイパーパラメーター最適化：スペクトルアプローチ（Hazan et al、2017）で</a>経験的に示されています-興味がある場合、私の履歴書は<a href="https://www.alexirpan.com/2017/06/27/hyperparam-spectral.html">こちら</a>です）。  NASはハイパーパラメーターを特に構成しませんが、ニューラルネットワークの設計決定がこの方法で行われることは非常に合理的だと思います。 ソリューションとパフォーマンスの間に強い相関関係があるため、これは学習にとって朗報です。 最後に、ここでは豊富な報酬だけでなく、モデルを教える際に私たちにとって重要なことはまさにここにあります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これらの理由から、他の環境で必要な数百万の例と比較して、NASが最適なニューラルネットワークを決定するために約12,800のトレーニング済みネットワークのみを必要とする理由が明らかになります。 さまざまな側面から、すべてがRLに有利に働きます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     一般に、同様の成功事例は依然として例外であり、規則ではありません。 強化された学習が説得力を持って機能するためには、パズルの多くの部分を正しく形成する必要があり、それでも困難です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     一般に、ディープRLはすぐに使用できるテクノロジーではありません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 未来を見る </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     古いことわざがあります-時間の経過とともに、すべての研究者は自分の研究分野を憎む方法を学びます。 冗談は、彼らが問題をあまりにも好きなので、研究者がまだこれをし続けているということです。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     それは、強化されたディープラーニングについて私が感じることです。 上記のすべてにもかかわらず、私は、RLが効果的でない可能性があるものを含むさまざまな問題にRLを使用することを試みる必要があることを絶対に確信しています。 しかし、RLを他にどのように改善できますか？ 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     技術に改善の時間が与えられている場合、ディープRLが将来機能しない理由はありません。 深いRLが広範囲に使用できるほど信頼できるようになると、いくつかの非常に興味深いことが起こり始めます。 問題はこれを達成する方法です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     以下に、もっともらしい将来の開発オプションをリストしました。 この方向に発展するためにさらなる研究が必要な場合、これらの分野の関連する科学論文へのリンクが示されます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>局所的な最適条件で十分</b>です。 人々自身がすべてにおいてグローバルに最適であると主張するのはar慢です。 私たちは文明を作成するために最適化された他の種よりわずかに優れていると思います。 同じ精神で、ローカルソリューションが個人の基本レベルを超えている場合、RLソリューションはグローバルオプティマを追求する必要はありません。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>鉄がすべてを決定します</b> 。  AIを作成するための最も重要なことは、単に鉄の速度を上げることだと信じている人々を知っています。 個人的に、私は鉄がすべての問題を解決するのではないかと疑っていますが、確かに重要な貢献をするでしょう。 すべてが高速に動作するほど、サンプルの非効率性に対する懸念が少なくなり、インテリジェンスの問題を通じてブルートフォースを突破しやすくなります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>さらに学習キューを追加します</b> 。 何が効果を正確に与えるかについての情報がほとんどないため、まばらな報酬を同化することは困難です。 幻覚（ <a href="https://arxiv.org/abs/1707.01495">Hindsight Experience Replay、Andrychowicz et al、NIPS 2017</a> ）の形で肯定的な報酬を生成するか、支援タスク（ <a href="https://arxiv.org/abs/1611.05397">UNREAL、Jaderberg et al、NIPS 2016</a> ）を定義するか、自己制御トレーニングから始まる世界の良いモデルを構築することが可能です。 いわば、チェリーをケーキに追加します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>モデルベースのトレーニングにより、サンプルの効率が向上し</b>ます。 モデルに基づいてRLを説明する方法は次のとおりです。「誰もがやりたいと思っていますが、その方法はほとんどわかっていません。」 原則として、優れたモデルは多くの問題を修正します。  AlphaGoの例に見られるように、モデルの存在は原則として、優れたソリューションの検索を非常に容易にします。 世界の良いモデルは新しいタスクにうまく移され、世界のモデルの導入により、新しい経験を想像することができます。 私の経験では、モデルベースのソリューションでは必要なサンプルも少なくなります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     しかし、良いモデルを訓練することは難しいことです。 低次元の状態モデルが時々機能するという印象を受けましたが、通常、画像モデルは難しすぎます。 しかし、それらがより簡単になると、いくつかの興味深いことが起こります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Bjsessionid%3D711FEF6BA26BBF98C28BC111B26F8761%3Fdoi%3D10.1.1.48.6005%26rep%3Drep1%26type%3Dpdf">Dyna（Sutton、1991）</a>および<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications_files/dyna2.pdf">Dyna-2（Silver et al。、ICML 2008）</a>は、この分野の古典作品です。 モデルベースの学習を深層ネットワークと組み合わせる作業の例として、バークレーロボティクス研究所の最近の記事をいくつかお勧めします。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <a href="http://bair.berkeley.edu/blog/2017/11/30/model-based-rl/">モデルフリーの微調整を使用したモデルベースのディープRLのニューラルネットワークダイナミクス（Nagabandi et al、2017</a> ; </li><li>  <a href="https://arxiv.org/abs/1710.05268">時間的スキップ接続を使用した自己管理視覚計画（Ebert et al、CoRL 2017）</a> ; </li><li>  <a href="https://arxiv.org/abs/1703.03078">軌道中心強化学習のためのモデルベースの更新とモデルフリーの更新の組み合わせ（Chebotar et al、ICML 2017）</a> ; </li><li>  <a href="http://rll.berkeley.edu/dsae/dsae.pdf">視覚運動学習のための深空間オートエンコーダー（フィンら、ICRA 2016）</a> ; </li><li>  <a href="http://jmlr.org/papers/v17/15-522.html">ガイド付きポリシー検索（Levine et al、JMLR 2016）</a> 。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>強化学習の使用は、微調整として簡単</b>です。 最初のAlphaGoの記事は、教師のトレーニングとRLの微調整から始まりました。 これは、より高速だが強力ではない方法を使用して初期トレーニングを高速化するため、適切なオプションです。 この方法は、異なるコンテキストでも機能しました<a href="https://arxiv.org/abs/1611.02796">-Sequence Tutor（Jaques et al、ICML 2017）を</a>参照してください。 別のシステムがこの「事前」の作成に関与している場合、確率（無作為）ではなく、合理的な事前分布のRLプロセスの開始と見なすことができます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>報酬関数は学習可能になり</b>ます。 機械学習は、データに基づいて、人が設計したものよりも優れたものを構築することを学ぶことができることを約束します。 報酬関数を選択することが非常に難しい場合、このタスクに機械学習を使用してみませんか？ シミュレーション学習とRLの反対-これらの豊富な領域は、報酬関数が人からの確認または評価によって暗黙的に決定できることを示しています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     逆RLおよびシミュレーショントレーニングに関する最も有名な科学論文は<a href="http://ai.stanford.edu/~ang/papers/icml00-irl.pdf">、Inverse Reinforcement Learning（Ng and Russell、ICML 2000）のアルゴリズム、Inverse Reinforcement</a> <a href="http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf">Learningを介した実習（Abbeel and Ng、ICML 2004）</a>および<a href="https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf">DAgger（Ross、Gordon、and Bagnell、AISTATS）です。 2011）</a> 。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これらのアイデアを深層学習の分野に拡大する最近の作品には、 <a href="https://arxiv.org/abs/1603.00448">ガイド付きコスト学習（Finnなど、ICML 2016）</a> 、 <a href="https://arxiv.org/abs/1704.06888">時間制約ネットワーク（Sermanet他、2017）</a> 、および<a href="https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/">Learning From Human Preferences（Christiano他、NIPS 2017）が含まれ</a>ます。 特に、リストされている最後の記事は、人々によって付けられた評価に由来する報酬が、実際にトレーニングされた元のハードコーディングされた報酬よ​​りも優れていることを示しています-これは良い実用的な結果です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     詳細なトレーニングを使用しない長期的な仕事の中で、私は<a href="https://arxiv.org/abs/1711.02827">Inverse Reward Design（Hadfield-Menell et al、NIPS 2017）</a>と<a href="http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf">物理的な人間の相互作用からの学習ロボット目標（Bajcsy et al、CoRL 2017）の記事が気に入りました</a> 。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>転送を学習し</b>ます。 学習の転送は、以前のタスクの知識を使用して新しいタスクの学習をスピードアップできることを約束します。<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">トレーニングが異種タスクを解決するのに十分に信頼できるようになるとき、私はこれが未来であると絶対に確信しています。まったく勉強できない場合はトレーニングを移管することは難しく、タスクAとBがある場合、タスクAからタスクBへのトレーニングの移管が起こるかどうかを予測することは困難です。私の経験では、ここに非常に明白な答えがあるか、まったく理解できません。そして、最も明白な場合でも、非自明なアプローチが必要です。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">この分野での最近の研究は、</font></font><a href="http://proceedings.mlr.press/v37/schaul15.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Universal Value Function Approximators（Schaul et al、ICML 2015）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font><a href="https://arxiv.org/abs/1707.04175"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distral（Whye Teh et al、NIPS 2017）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">および</font><a href="https://arxiv.org/abs/1707.04175"><font style="vertical-align: inherit;">Over </font></a></font><a href="https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">克服Catastrophic Forgetting（Kirkpatrick et al、PNAS 2017）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">です。古い作品については、</font></font><a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.297.6455%26rep%3Drep1%26type%3Dpdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Horde（Sutton et al、AAMAS 2011）を</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参照してください</font><font style="vertical-align: inherit;">。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">たとえば、ロボット工学は、シミュレーターから実世界へのトレーニングの転送（タスクのシミュレーションから実際のタスクへ）で順調に進歩しています。参照してください。</font></font><a href="https://blog.openai.com/spam-detection-in-the-physical-world/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ザ・ドメインのランダム化を（トービンら、2017 IROS） </font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font><a href="https://arxiv.org/abs/1610.04286"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">シム・ツー・実ロボットネッツプログレッシブと学習（Rusuのら、Corl 2017）</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">と</font></font><a href="https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GraspGAN（Bousmalisら、2017） </font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">（免責事項：私はGraspGANに取り組みました）。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">良い事前知識は、トレーニング時間を大幅に短縮できます。</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。これは、以前のポイントのいくつかと密接に関連しています。一方で、学習の移転は、過去の経験を使用して、他のタスクの事前確率分布を作成することです。 RLアルゴリズムは、マルコフの意思決定プロセスで動作するように設計されています。ここで、一般化に問題があります。私たちのソリューションが環境の狭いセクターでのみうまく機能すると信じるなら、これらの環境を効果的に解決するために共通の構造を使用できるはずです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pieter Ebbillは、スピーチの中で、現実の世界で解決するようなタスクのみに深いRLを要求する必要があることに注目しています。これは非常に理にかなっていることに同意します。非現実的なタスクの学習を遅くすることで、新しい実際のタスクをすばやく学習できるように、実世界の事前知識が必要です。これは完全に受け入れられる妥協案です。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">困難なのは、このような実世界の事前の設計が非常に難しいことです。ただし、これがまだ可能である可能性は十分にあると思います。個人的には、データから合理的な事前分布を生成する方法を提供するため、メタトレーニングに関する最近の研究に満足しています。たとえば、RLを使用して倉庫をナビゲートする場合、まずメタトレーニングを使用して一般的なナビゲーションを教え、次にロボットが移動する特定の倉庫に対して事前にこれを微調整することは興味深いでしょう。これは未来に非常に似ており、問題はメタトレーニングがそこに到達するかどうかです。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最新の学習学習作業の概要については、</font></font><a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">BAIR（Berkeley AI Research）のこの出版物を</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参照してください</font><font style="vertical-align: inherit;">。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">より複雑な環境は逆説的に簡単になります</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。パルクールボットに関するDeepMindの記事の主な結論の1つは、タスクのバリエーションをいくつか追加することでタスクを非常に複雑にすると、実際にはトレーニングを簡素化できるということです。ルールは他のすべてのパラメーターのパフォーマンスを損なうことなく1つの設定で再トレーニングできないためです。ドメインランダム化に関する記事やImageNetでも同様のことがわかりました。ImageNetでトレーニングされたモデルは、CIFAR-100でトレーニングされたモデルよりもはるかに優れた他の環境に拡張できます。先ほど言ったように、もっと普遍的なRLに移行するために、「管理用のImageNet」を作成するだけで十分でしょう。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多くのオプションがあります。</font></font><a href="https://github.com/openai/gym"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAIジムは</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最も人気のある環境ですが、</font></font><a href="https://github.com/mgbellemare/Arcade-Learning-Environment"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">アーケード学習環境</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font><a href="https://github.com/openai/roboschool"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Roboschool</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font><a href="https://github.com/deepmind/lab"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMind Lab</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">、</font></font><a href="https://github.com/deepmind/dm_control"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMind Control Suite、</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">および</font></font><a href="https://github.com/facebookresearch/ELF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELF</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">最後に、これは学術的な観点からはs辱的ですが、深いRLの経験的な問題は実際的な観点からは問題ではないかもしれません。架空の例として、金融会社がディープRLを使用しているとします。彼らは、3つのランダムなsidを使用して、過去の米国株式市場データについて販売代理店を訓練します。実際のA / Bテストでは、最初のシードは2％少ない収入をもたらし、2番目のシードは平均収益性で機能し、3番目のシードは2％増加します。この仮想バージョンでは、再現性は重要ではありません-歩留まりが2％高いモデルを展開して喜ぶだけです。同様に、販売代理店が米国でのみうまく機能することは問題ではありません。世界市場でうまく機能しない場合は、そこでは使用しないでください。異常なシステムと再現可能な異常なシステムには大きな違いがあります。おそらく、最初に集中する必要があります。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 今どこにいるの </font></font></h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">多くの点で、ディープRLの現在の状態に悩まされています。</font><font style="vertical-align: inherit;">それにもかかわらず、それは研究者の間でそのような強い関心を集めていますが、これは私が他の分野で見たことはありません。</font><font style="vertical-align: inherit;">私の気持ちは、アンドリュー・ウンが</font></font><a href="https://www.youtube.com/watch%3Fv%3DF1ka6a13S9I"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ディープラーニングを適用するナッツとボルトとの</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">彼の講演で言及したフレーズによって最もよく表され</font><font style="vertical-align: inherit;">ます：短期的な強い悲観主義、さらに強い長期的な楽観主義とのバランス。</font><font style="vertical-align: inherit;">ディープRLは少し混chaとしていますが、私はまだ将来を信じています。</font></font>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ただし、強化学習（RL）で問題を解決できるかどうか再度尋ねられた場合、私はすぐに「いいえ」と答えます。</font><font style="vertical-align: inherit;">しかし、数年後にこの質問を繰り返すようお願いします。</font><font style="vertical-align: inherit;">それまでに、おそらくすべてがうまくいくでしょう。</font></font></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../J349790/index.html">Dart 2.0のお知らせ：クライアント開発向けに最適化</a></li>
<li><a href="../J349794/index.html">Magento Meetup Kharkov-ビデオとプレゼンテーション</a></li>
<li><a href="../J349796/index.html">DevOps（および最新のソフトウェア）が好きではない理由</a></li>
<li><a href="../J349798/index.html">忘れられたトランザクションの自動削除</a></li>
<li><a href="../J34980/index.html">PDAにはHabrahabrのバージョンが必要ですか？</a></li>
<li><a href="../J349802/index.html">Dockerのマルチステージビルド</a></li>
<li><a href="../J349804/index.html">同僚との一連の会話または少しの経験から：DC Edgeの設計</a></li>
<li><a href="../J349808/index.html">どうやってAB-DOCをしますか</a></li>
<li><a href="../J349810/index.html">React / Reduxを使用する難民のためのHyperapp</a></li>
<li><a href="../J349812/index.html">ダミーのための分離された証人</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter70218013 = new Ya.Metrika({
                  id:70218013,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/70218013" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'G-FEDBM7F51Q', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Clever Geek | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <div class="company-info js-company-info" itemscope="" itemtype="http://schema.org/Organization">
      <span itemprop="name">Western Town Media (WTM)</span>
      <div itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress">
        <span itemprop="streetAddress">1968 Stoney Lonesome Road</span>
        <br>
        <span itemprop="postalCode">PA 18640</span>
        <span itemprop="addressLocality">Pittston, USA</span>
      </div>
      <span itemprop="telephone">570-362-1316</span>
    </div>
    <script type="application/ld+json">
      {
        "@context": "http://schema.org",
        "@type": "Organization",
        "address": {
          "@type": "PostalAddress",
          "addressLocality": "Pittston, USA",
          "postalCode": "PA 18640",
          "streetAddress": "1968 Stoney Lonesome Road"
        },
        "name": "Western Town Media (WTM)",
        "telephone": "570-362-1316"
      }
    </script>
  </div>
</footer>
  
</body>

</html>
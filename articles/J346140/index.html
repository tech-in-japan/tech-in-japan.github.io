<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FEDBM7F51Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FEDBM7F51Q');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤘 👩‍🎨 👩🏼‍🤝‍👩🏻 1年間のコンピュータービジョンの開発結果 👩🏽‍🤝‍👩🏻 🤾🏼 👩🏽‍🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="パート1 分類/位置特定、オブジェクト検出、オブジェクト追跡 
  
 
  
  このスニペットは、コンピュータービジョン研究チームによる最近の出版物から引用されています。 今後数か月で、この驚くべき技術とその現状についてさらに学びたい人に教育リソースを提供するために、人工知能の分野のさまざまな研...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="sitemap" type="application/xml" href="/sitemap.xml"/>

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

  <script>document.write('<script src="https://pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://tech-in-japan.github.io/index.html"></a>
    <div class="page-header-text">Clever Geek Handbook</div>
  </header>
  <section class="page js-page"><h1>1年間のコンピュータービジョンの開発結果</h1><div class="post__text post__text-html js-mediator-article" id="post-content-body" data-io-article-url="https://habr.com/ru/post/346140/">  <b>パート1</b>  <b>分類/位置特定、オブジェクト検出、オブジェクト追跡</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote>  <i>このスニペットは、コンピュータービジョン研究チームによる最近の出版物から引用されています。</i>  <i>今後数か月で、この驚くべき技術とその現状についてさらに学びたい人に教育リソースを提供するために、人工知能の分野のさまざまな研究トピックに関する作品を、その経済的、技術的、社会的応用について公開します。</i>  <i>私たちのプロジェクトは、すべての研究者に最先端のAI開発に関する情報を提供する成長する仕事に貢献したいと考えています。</i> </blockquote>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> はじめに </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     通常、コンピュータービジョンは科学的分野と呼ばれ、マシンに視覚的またはよりカラフルな能力を与え、マシンが周囲の環境やインセンティブを視覚的に分析できるようにします。 このプロセスには通常、1つ以上の画像またはビデオの評価が含まれます。  British Machine Vision Association（BMVA） <a href="http://www.bmva.org/visionoverview">は、</a>コンピュータービジョンを<i>「画像またはそのシーケンスから有用な情報を自動的に抽出、分析、および理解する」と</i> <a href="http://www.bmva.org/visionoverview">定義</a> <i>しています</i> 。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>理解</i>という用語は、視覚の機械的定義の背景に対して興味深い<i>ことに</i>際立っており、コンピュータービジョンの分野の重要性と複雑さの両方を示しています。 私たちの周囲の真の理解は、視覚的表現を通して達成されるだけではありません。 実際、視覚信号は視神経を通過して一次視覚野に入り、高度に定型化された感覚で脳によって解釈されます。 この感覚情報の解釈は、私たちの自然な組み込みプログラムと主観的経験のほぼすべてのセット、つまり、進化がどのように生き残るようにプログラムしたか、そして私たちが生涯を通じて世界について学んだことをカバーします。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     この点で、 <i>ビジョン</i>とは解釈のための画像の送信のみを指します。 また、 <i>コンピューティング</i>は、画像が思考や意識に似ていることを示し、脳の多くの能力に依存しています。 したがって、多くの人は、コンピューター環境、つまり視覚環境とそのコンテキストの真の理解が、クロスドメイン領域での完璧な習熟のおかげで、強力な人工知能の将来のバリエーションに道を開くと信じています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     しかし、この驚くべき領域の開発の初期段階を実質的に残していないので、武器をつかまないでください。 この記事では、2016年のコンピュータービジョンの最も重要な成果について簡単に説明します。 そして、これらの成果のいくつかを、予想される短期の社会的相互作用と、該当する場合、私たちが知っているように人生の終わりの仮説的予測の強固な混合物に当てはめることを試みることができます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     私たちの仕事は常に可能な限り最もアクセスしやすい方法で書かれていますが、この特定の記事のセクションは議論の主題のために少しあいまいに見えるかもしれません。 私たちはどこでも原始的なレベルで定義を提供していますが、それらは重要な概念の表面的な理解のみを提供します。  2016年の作品に焦点を当て、簡潔にするために省略していることがよくあります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     これらの明らかな省略の1つは、コンピュータービジョンの分野で広く使用されている畳み込みニューラルネットワーク（CNN）の機能に関連しています。  ImageNetコンテストで競合他社を驚かせた2012年の<a href="http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf">AlexNet</a>の成功、CNNアーキテクチャは、この分野で事実上起こった革命の証拠でした。 その後、多数の研究者がCNNベースのシステムの使用を開始し、畳み込みニューラルネットワークがコンピュータービジョンの伝統的な技術になりました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      4年以上が経過し、コンピュータービジョン用の新しいニューラルネットワークアーキテクチャの大部分はCNNバリアントによって構成されています。 研究者は、それらをデザイナーキューブのように作り直します。 これは、オープンソースの科学出版物とディープラーニングの両方の力の真の証拠です。 ただし、畳み込みニューラルネットワークの説明はいくつかの記事に簡単に拡張されるため、主題をより深く理解し、複雑なものを明確な言語で説明したい人に任せたほうがよいでしょう。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     この記事を続ける前にトピックをすばやく理解したい一般の読者には、以下の最初の2つの情報源をお勧めします。 主題をさらに深く掘り下げたい場合は、他のソースを提供します。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Andrey Karpatyによる「 <a href="http://karpathy.github.io/2015/10/25/selfie/">ディープニューラルネットワークが自分のセルフィーをどう思うか</a> 」は、畳み込みニューラルネットワークのアプリケーションと機能を理解するのに役立つ最高の記事の1つです。 </li><li>  <a href="https://www.quora.com/What-is-a-convolutional-neural-network">Quora：「畳み込みニューラルネットワークとは」</a>すばらしいリンクと説明が満載です。  <u>この分野で事前に理解していない</u>人に特に適しています。 </li><li>  <a href="http://cs231n.stanford.edu/">CS231n：スタンフォード大学の「視覚認識のための畳み込みニューラルネットワーク」</a>は、このトピックをより深く研究するための優れたリソースです。 </li><li>  <a href="http://www.deeplearningbook.org/">ディープラーニング</a> （Goodfellow、Bengio＆Courville、2016）は、 <a href="http://www.deeplearningbook.org/contents/convnets.html">第9章</a>で畳み込みニューラルネットワークの機能と機能の詳細な説明を提供しています。 著者は、このチュートリアルをHTML形式で無料で公開しています。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ニューラルネットワークとディープラーニング全般をより完全に理解するには、以下をお勧めします。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <a href="http://neuralnetworksanddeeplearning.com/index.html">ニューラルネットワークとディープラーニング</a> （Nielsen、2017年）は、 <a href="http://neuralnetworksanddeeplearning.com/index.html">ニューラルネットワークとディープラーニング</a>のすべての複雑さを真に直感的に理解できる無料のオンラインチュートリアルです。 最初のパートを読んでも、初心者向けにこの記事のトピックを大きく照らすはずです。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     一般に、この記事は断片的でけいれん的であり、著者の賞賛とそれがどのように使用されるべきかの精神をセクションごとに反映しています。 情報は、私たち自身の発見的手法と判断に従って部分に分割されます。これは、このような多数の科学論文のクロスドメインの影響による必要な妥協です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     読者が情報の一般化から利益を得て、以前の荷物に関係なく知識を向上させることを願っています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     すべての参加者を代表して、 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>Mタンク</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a36/f2d/947/a36f2d947ce1ef4dd7cad5bff68faf97.png"></div>
      
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 分類/ローカリゼーション </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     画像に関する分類タスクは、通常、画像全体にラベルを付けることです（たとえば、「猫」）。 これを念頭に置いて、ローカライズとは、この画像内のオブジェクトの場所を特定することを意味します。 通常、オブジェクトの周囲の特定の境界ボックスで示されます。  <a href="http://image-net.org/challenges/LSVRC/2016/index">ImageNetの現在の分類方法は</a> 、オブジェクト分類の精度において、 <a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">特別に訓練された人々のグループより</a> <a href="http://image-net.org/challenges/LSVRC/2016/index">も</a>すでに<a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">優れて</a>います。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>図</b>  <b>1</b> ：コンピュータービジョンタスク 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/73c/3a7/a1c/73c3a7a1cd66c7d32a7c62b0fbe9fe74.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><font color="gray"><b>出典</b> ：フェイフェイ、アンドレイカルパシー＆ジャスティンジョンソン（2016）cs231n、レクチャー8-スライド8、空間的位置確認と検出（2016年1月2日）、 <a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf">pdf</a></font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ただし、クラスの数が増えると、近い将来の進捗を測定するための新しいメトリックが提供される可能性があります。 特に、Kerasの作成者であるFrançoisScholletは、人気のある<a href="https://arxiv.org/abs/1610.02357v2">Xception</a>アーキテクチャなどの<a href="https://arxiv.org/abs/1607.05691v1">新しいメソッドを</a> 、17,000クラスを含む複数のラベルを持つ3億5,000万枚以上の画像を含むGoogleの内部データセットに適用しました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>図</b>  <b>2</b> ：ILSVRCコンテスト（2010〜2016年）の分類/ローカリゼーション結果 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/363/7ae/07c/3637ae07cacb4933099119766eb908f7.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><font color="gray"><b>注</b> ：ImageNet大規模視覚認識チャレンジ（ILSVRC）。</font></i>  <i><font color="gray">2011-2012以降の改善は、AlexNetの登場によるものです。</font></i>  <i><font color="gray">分類とローカリゼーションの競合要件の<a href="http://www.image-net.org/challenges/LSVRC/2016/index">概要を</a>ご覧ください。</font></i> <i><font color="gray">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </font></i>  <i><font color="gray"><b>出典</b> ：Jia Deng（2016）。</font></i>  <i><font color="gray">ILSVRC2016オブジェクトのローカライズ：紹介、結果。</font></i>  <i><font color="gray">スライド2、 <a href="http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf">pdf</a></font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>ImageNet LSVRC（2016）の興味深い抜粋：</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b>シーンの分類とは</b> 、温室、スタジアム、大聖堂など、特定のクラスのシーンで画像にラベルを付けるタスクのことです。  ImageNetは、 <a href="http://places2.csail.mit.edu/">Places2データセットの</a>サンプルのシーンを分類するためのコンテストを昨年開催しました。365個のシーンカテゴリでトレーニングするための800万枚の画像です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/">Hikvision</a>は、上位5つのエラーの9％で勝ちました。 このシステムは、インセプションスタイルのディープニューラルネットワークとそれほど深くない残差ネットワークのセットから構築されます。 </li><li>  <b>Trimps-Soushen</b>は2.99％のトップ5の分類エラーと7.71％のローカリゼーションエラーでImageNet分類チャレンジを獲得しました。 開発者は、いくつかのモデル（Inception、Inception-Resnet、ResNet、およびWide Residual Networksモデルの結果を平均化する）のシステムをコンパイルし、タグによるローカリゼーションで<a href="http://image-net.org/challenges/LSVRC/2016/results">Faster R-CNN</a>が勝ちました。 データセットは、トレーニング用に120万の画像を含む1000の画像クラスに分散されました。 テストデータセットには、ニューラルネットワークがこれまで見たことのない別の10万枚の画像が含まれていました。 </li><li>  Facebookの<b><a href="https://arxiv.org/abs/1611.05431v2">ResNeXt</a></b>ニューラルネットワークは、3.03％のトップ5の分類エラーで2番目にわずかなマージンで終了しました。 元のResNetアーキテクチャを拡張する新しいアーキテクチャを使用しました。 </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> 物体検出 </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ご想像のとおり、オブジェクトを検出するプロセスは、本来行うべきことを正確に実行します。画像内のオブジェクトを検出します。  <a href="http://image-net.org/challenges/LSVRC/2016/%2523det">ILSVRC 2016オブジェクト検出定義に</a>は、個々のオブジェクトの境界ボックスとラベルの発行が含まれます。 これは、分類とローカリゼーションが1つの支配的なオブジェクトではなく、多くのオブジェクトに適用されるため、分類/ローカリゼーションタスクとは異なります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>図</b>  <b>3</b> ：顔が唯一のクラスであるオブジェクトの検出 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/908/233/59b/90823359b74d42dbe5823cff96b1eb7f.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><font color="gray"><b>注</b> ：写真は、同じクラスのオブジェクトの検出としての顔検出の例です。</font></i>  <i><font color="gray">著者は、この分野で絶え間ない問​​題の1つを小さな物体の検出と呼んでいます。</font></i>  <i><font color="gray">テストクラスとして小さな顔を使用して、彼らはサイズ、画像解像度、およびコンテキストの正当化における不変性の役割を調査しました。</font></i> <i><font color="gray">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </font></i>  <i><font color="gray"><b>出典</b> ： <a href="https://arxiv.org/abs/1612.04402v1">Hu、Ramanan（2016、p。1）</a></font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      2016年のオブジェクト検出の分野における主な傾向の1つは、より高速で効率的な検出システムへの移行でした。 これは、YOLO、SSD、R-FCNなどのアプローチで、イメージ全体の共同コンピューティングへのステップとして見ることができます。 これは、Fast / Faster R-CNNテクニックに関連するリソース集約型サブネットとは異なります。 この手法は、一般的にエンドツーエンドのトレーニング/学習と呼ばれます。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     本質的に、アイデアは互いに独立した各サブ問題の個別のアルゴリズムの使用を避けることです。これは通常、トレーニング時間を増加させ、ニューラルネットワークの精度を低下させるためです。 このようなニューラルネットワークの最初から最後まで機能する適応は、通常、最初のサブネットが機能した後に発生するため、遡及的な最適化を表すと言われています。 ただし、高速/高速R-CNN技術は依然として非常に効率的であり、オブジェクト検出に広く使用されています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b><a href="https://arxiv.org/abs/1512.02325v5">SSD：シングルショットMultiBox Detector</a></b>は、必要なすべての計算を実行する単一のニューラルネットワークを使用し、前世代のリソースを集中的に使用する方法を不要にします。  「75.1％mAP、同等の最先端のFaster R-CNNを上回る」を実証しています。 </li><li>  2016年の最も印象的な開発の1つは、 <b><a href="https://arxiv.org/abs/1612.08242v1">YOLO9000</a></b>という適切な名前のシステムです<b><a href="https://arxiv.org/abs/1612.08242v1">。Better、Faster、Stronger</a></b>で、YOLOv2およびYOLO9000検出システムを使用します（YOLOはYou Only Look Onceを意味します）。  YOLOv2は<a href="https://arxiv.org/abs/1506.02640v5">2015年半ばから</a>大幅に改良された<a href="https://arxiv.org/abs/1506.02640v5">YOLOモデル</a>であり、非常に高いフレームレート（通常のGTX Titan Xを使用した場合の低解像度画像で最大90 FPS）のビデオでより良い結果を表示できます。 システムは、速度の向上に加えて、オブジェクトを識別するための特定のデータセットでResNetおよびSSDを使用したFaster RCNNを超えています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      YOLO9000は、オブジェクトを検出および分類するための複合学習方法を実装し、利用可能なラベル付き検出データを超えて予測機能を拡張します。 つまり、タグ付きデータで検出されたことのないオブジェクトを検出できます。  YOLO9000モデルは、9000を超えるカテゴリ間でリアルタイムのオブジェクト検出を提供します。これにより、分類と検出のデータセットのサイズの違いがなくなります。 詳細、事前トレーニング済みモデル、およびビデオデモについては、こちらをご覧ください。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>オブジェクト検出YOLOv2は、James Bondのムービーフレームで実行されます</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/VOC3huqHrss" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></li><li>  <b><a href="https://arxiv.org/abs/1612.03144v1">オブジェクト検出用の機能ピラミッドネットワークは</a></b> 、FAIR（Facebook Artificial Intelligence Research）によって開発されました。  <i>「深層畳み込みニューラルネットワークの生得的なマルチスケールピラミッド階層を</i>使用して、 <i>最小限の追加コストでフィーチャピラミッドを構築します</i> 。 <i>」</i> これは、速度や追加のメモリオーバーヘッドを犠牲にすることなく、強力な表現を維持することを意味します。 開発者は、COCO（コンテキスト内の共通オブジェクト）データセットでレコードレベルを達成しています。  Faster R-CNNベースシステムと組み合わせると、2016年の勝者よりも優れています。 </li><li>  <b><a href="https://arxiv.org/abs/1605.06409v2">R-FCN：領域ベースの完全畳み込みネットワークによるオブジェクト検出</a></b> 。 開発者が、各画像で何百回も画像の個々の領域にリソースを集中的に使用するサブネットの使用を放棄する別の方法。 ここで、領域検出器は完全に畳み込み式であり、画像全体に対して共同計算を実行します。  <i>「テスト中、動作速度はイメージごとに170ミリ秒でした。これは、高速R-CNNよりも2.5〜20倍高速です」と</i>著者は書いています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>図</b>  <b>4</b> ：異なるアーキテクチャ上のオブジェクトを検出する場合のオブジェクトの精度とサイズのトレードオフ 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/ea8/c2a/c36/ea8c2ac365880ad2856c071017582184.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><font color="gray"><b>注</b> ：mAP（平均平均精度）は垂直軸にプロットされ、各特徴抽出ブロックのさまざまなメタアーキテクチャが水平軸（VGG、MobileNet ... Inception ResNet V2）にプロットされます。</font></i>  <i><font color="gray">また、小、中、大のmAPは、それぞれ小、中、大のオブジェクトの平均精度を示します。</font></i>  <i><font color="gray">基本的に、精度はオブジェクトのサイズ、メタアーキテクチャ、および特徴抽出ユニットに依存します。</font></i>  <i><font color="gray">この場合、「画像サイズは300ピクセルに固定されます。」</font></i>  <i><font color="gray">この例では、R-CNNの高速モデルは比較的良好に機能しましたが、このメタアーキテクチャは、R-FCNなどの最新のアプローチよりも大幅に遅いことに注意することが重要です。</font></i> <i><font color="gray">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </font></i>  <i><font color="gray"><b>出典</b> ： <a href="https://arxiv.org/abs/1611.10012v1">Huang et al。</a></font></i>  <i><font color="gray"><a href="https://arxiv.org/abs/1611.10012v1">（2016年、9ページ）</a></font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     上記の<a href="https://arxiv.org/abs/1611.10012v1">科学記事</a>は、R-FCN、SSD、および高速R-CNNのパフォーマンスの詳細な比較を提供します。 機械学習の手法を正確に比較するのは難しいため、著者が説明した標準化されたアプローチを作成するメリットを指摘したいと思います。 彼らは、これらのアーキテクチャをResmetやInceptionなどのさまざまな特徴抽出ユニットと組み合わせることができるため、「メタアーキテクチャ」と見なしています。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     著者は、さまざまなメタアーキテクチャ、特徴抽出ブロック、および解像度における精度と速度のトレードオフを研究しています。 たとえば、特徴抽出ユニットを選択すると、さまざまなメタアーキテクチャの作業結果が大きく変わります。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="https://arxiv.org/abs/1612.01051v2">SqueezeDet</a>と<a href="https://arxiv.org/abs/1611.08588v2">PVANet</a>を説明している科学記事は、消費されるコンピューティングリソースを削減しながら、アプリケーションの速度を上げる傾向と、特に無人車両アプリケーションで商用リアルタイムアプリケーションに必要な精度を維持する傾向との間の妥協の必要性を再度強調しています。 中国企業のDeepGlintは、監視カメラからのストリームでリアルタイムにオブジェクトを検出する良い例を示しました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>DeepGlintでのオブジェクト検出、オブジェクト追跡、顔認識</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/xhp47v5OBXQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>ILSVRCおよびCOCO検出チャレンジの結果</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="http://mscoco.org/">COCO</a> （コンテキスト内の共通オブジェクト）は、別の一般的な画像データのセットです。 ただし、ImageNetなどの代替製品よりも比較的小さく、慎重にキュレーションされています。 シーンを理解するためのより広いコンテキストでオブジェクトを認識することを目的としています。 主催者は、オブジェクトの検出、セグメンテーション、およびキーポイントをめぐって毎年コンテストを開催しています。 オブジェクト検出に関する<a href="http://image-net.org/challenges/LSVRC/2016/results">ILSVRC</a>および<a href="http://mscoco.org/dataset/%2523detections-leaderboard">COCO</a>コンテストの結果は次の<a href="http://image-net.org/challenges/LSVRC/2016/results">とおり</a>です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b>ImageNet LSVRC、画像検出（DET）</b> ：CUImageは66％meanAPを示しました。 オブジェクトの200のカテゴリのうち109で勝ちました。 </li><li>  <b>ImageNet LSVRC、ビデオ上のオブジェクト検出（VID）</b> ：NUIST 80.8％meanAP </li><li>  <b>ImageNet LSVRC、追跡ビデオのオブジェクト検出</b> ：CUvideo 55.8％meanAP </li><li>  <b>COCO 2016、オブジェクト検出（境界ボックス）</b> ：G-RMI（Google）41.5％AP（2015年の受賞者と比較して4.2パーセントポイントの絶対的な増加-MSRAVC） </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      ImageNet <a href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">は</a> 、2016年のオブジェクト検出システムによって示された結果のレビューで、MSRAVC 2015が非常に高いパフォーマンスバーを設定したと<a href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">書い</a>ています（このコンテストでResNetネットワークが初めて登場しました）。 システムのパフォーマンスはすべてのクラスで改善されました。 どちらのコンテストでも、ローカライズは大幅に改善されました。 小さなオブジェクトで大幅な改善が達成されました。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>図</b>  <b>5</b> ：ILSVRCコンペティションにおける画像検出システムの結果（2013–2016） 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/17d/d6c/5d3/17dd6c5d391f4be0aadb4b834252fe83.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><font color="gray"><b>注</b> ：ILSVRCコンペティション（2013–2016）の画像検出システムの結果。</font></i>  <i><font color="gray"><b>出典</b> ：ImageNet 2016、オンラインプレゼンテーション、スライド2、 <a href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">pdf</a></font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1> オブジェクト追跡 </h1>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     特定の対象オブジェクトまたは特定のシーン上の複数のオブジェクトを追跡するプロセスを指します。 従来、このプロセスは、ビデオアプリケーションおよび実世界との相互作用のシステムで使用され、ソースオブジェクトの発見後に観察が行われます。 たとえば、このプロセスは無人車両システムにとって重要です。 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li> <b>« <a href="https://arxiv.org/abs/1606.09549v2">      </a> »</b>       ,     ,            ,       .        ,        . </li><li> <b>« <a href="https://arxiv.org/abs/1604.01802v2">       100 FPS</a> »</b> —   ,           .   ,        (feed-forward network)         ,     .       <u>  </u> .       ,      «    100 FPS». 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <b>  GOTURN (Generic Object Tracking Using Regression Networks)</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/kMhwXnLgT_I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></li><li>  <b>« <a href="https://arxiv.org/abs/1612.06615v1">     </a> »</b>    ,   RGB/  ( CNN),      (    ),    .              ,  ,       .     <u> </u>   ICPR 2016,   «    ». 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <i>«             .   ,       ,   RGB    .   ,               .    ,           ,        ».</i> </li><li>  <b>« <a href="https://arxiv.org/abs/1605.06457v1">         </a> »</b>                .             , , ,      .      ,      .      ,       ,     . </li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「</font></font><a href="https://arxiv.org/abs/1612.08274v1"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">完全な畳み込みネットワークとオブジェクトのグローバル最適追跡</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">」</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">オブジェクト追跡システムの制限の2つの根本原因として、オブジェクトの多様性と干渉について説明します。</font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">「提案された方法は、完全な畳み込みネットワークを使用してオブジェクトの外観の多様性の問題を解決し、動的プログラミングを通じて干渉の問題とも連携します</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。</font><i><font style="vertical-align: inherit;">」</font></i></font></li></ul></li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../J346130/index.html">それでは、なぜあなたはオープンソースソフトウェアの開発に関与しないのですか？</a></li>
<li><a href="../J346132/index.html">Stimulus 1.0：既にお持ちのHTML用の謙虚なJavaScriptフレームワーク</a></li>
<li><a href="../J346134/index.html">アルゴリズムを構築するための遺伝的アルゴリズム</a></li>
<li><a href="../J346136/index.html">LÖVEでの開発</a></li>
<li><a href="../J346138/index.html">ゲームデザインの秘::ミキシング</a></li>
<li><a href="../J346142/index.html">Reactでのコンポーネントのホットリロード</a></li>
<li><a href="../J346144/index.html">内部からのモバイルデバイス。 メモリマークアップ、メモリ記述の構造、およびマークアップファイル</a></li>
<li><a href="../J346146/index.html">Pythonフラグ</a></li>
<li><a href="../J346152/index.html">IntelのCEOは、プロセッサの脆弱性のために2400万ドルの株を売ると疑われている</a></li>
<li><a href="../J346154/index.html">RxJでReactJSコンポーネントを簡素化する</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter70218013 = new Ya.Metrika({
                  id:70218013,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/70218013" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'G-FEDBM7F51Q', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Clever Geek | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2020</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <div class="company-info js-company-info" itemscope="" itemtype="http://schema.org/Organization">
      <span itemprop="name">Western Town Media (WTM)</span>
      <div itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress">
        <span itemprop="streetAddress">1968 Stoney Lonesome Road</span>
        <br>
        <span itemprop="postalCode">PA 18640</span>
        <span itemprop="addressLocality">Pittston, USA</span>
      </div>
      <span itemprop="telephone">570-362-1316</span>
    </div>
    <script type="application/ld+json">
      {
        "@context": "http://schema.org",
        "@type": "Organization",
        "address": {
          "@type": "PostalAddress",
          "addressLocality": "Pittston, USA",
          "postalCode": "PA 18640",
          "streetAddress": "1968 Stoney Lonesome Road"
        },
        "name": "Western Town Media (WTM)",
        "telephone": "570-362-1316"
      }
    </script>
  </div>
</footer>
  
</body>

</html>